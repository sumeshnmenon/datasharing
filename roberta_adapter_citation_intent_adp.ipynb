{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberta_adapter_citation_intent_adp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumeshnmenon/datasharing/blob/master/roberta_adapter_citation_intent_adp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dh0KstMKnBbq",
        "outputId": "d4afa954-7d63-4e2e-8a05-79789c011c4a"
      },
      "source": [
        "import tensorflow as tf \n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2_AL51qlYCf",
        "outputId": "7c47569d-d339-4cc1-a38b-51abc53ca0db"
      },
      "source": [
        "!pip install pip==19.3.1 "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip==19.3.1 in /usr/local/lib/python3.7/dist-packages (19.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1-W6IONlZtT",
        "outputId": "f5f65c41-63d0-4d42-a4b8-f76865c54c45"
      },
      "source": [
        "!pip install git+https://github.com/kernelmachine/allennlp.git@4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/kernelmachine/allennlp.git@4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7\n",
            "  Cloning https://github.com/kernelmachine/allennlp.git (to revision 4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7) to /tmp/pip-req-build-h5079sgo\n",
            "  Running command git clone -q https://github.com/kernelmachine/allennlp.git /tmp/pip-req-build-h5079sgo\n",
            "  Running command git checkout -q 4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (0.22.2.post1)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 4.3MB/s \n",
            "\u001b[?25hCollecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (0.5.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (2018.9)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (3.2.5)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (1.8.1+cu101)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (3.2.2)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (3.6.4)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.9MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/e5/96f7156e6ebf7ab992471479c3c55f0be2f31360fcdcac21aa6f782c036a/boto3-1.17.57-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.1MB/s \n",
            "\u001b[?25hCollecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/85/df3d1fd2b60a87455475f93012861b76a411d27ba4a0859939adbe2c9dc3/gevent-21.1.2-cp37-cp37m-manylinux2010_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (2.23.0)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (1.19.5)\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/a8/ba/03b4c978708510c2ab52a75804530edfd96647f3de44abe1cf25d16150ad/responses-0.13.2-py2.py3-none-any.whl\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 42.6MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 40.5MB/s \n",
            "\u001b[?25hCollecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (0.4.1)\n",
            "Collecting spacy<2.2,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/f3/554271be8ff46471586d164bfbb6999364ba30ca5a0045e2a86da5f3b2c5/spacy-2.1.9-cp37-cp37m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.8MB 101kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (2.10.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 38.8MB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/13/08/ffd2ebf3c1c5be9be493dcbdb250567e4b244953b061cad5c7f0160b28ff/overrides-4.1.1-py3-none-any.whl\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (4.41.1)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from allennlp===0.9.1-unreleased) (1.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp===0.9.1-unreleased) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp===0.9.1-unreleased) (2019.12.20)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.11.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp===0.9.1-unreleased) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->allennlp===0.9.1-unreleased) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp===0.9.1-unreleased) (20.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp===0.9.1-unreleased) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp===0.9.1-unreleased) (8.7.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp===0.9.1-unreleased) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp===0.9.1-unreleased) (56.0.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp===0.9.1-unreleased) (1.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp===0.9.1-unreleased) (3.10.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp===0.9.1-unreleased) (0.2.5)\n",
            "Collecting botocore<1.21.0,>=1.20.57\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/52/aa266c9594e279799ded419caac56365796ce686b97762b9c8620b2ba988/botocore-1.20.57-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 36.5MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.0MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp===0.9.1-unreleased) (1.0.0)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/a7/94e1a92c71436f934cdd2102826fa041c83dcb7d21dd0f1fb1a57f6e0620/zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (1.24.3)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (2.0.5)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 34.5MB/s \n",
            "\u001b[?25hCollecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/42/d7ea7539af3852fd8c1f0b3adf4a100fb3d72b40b69cef1a764ff979a743/thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 36.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (0.8.2)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (1.0.5)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (1.0.5)\n",
            "Collecting typing-utils>=0.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/d6/ed54e008ae5aa828b77089aa371e25cb1313271abebd18d20e650fe903b8/typing_utils-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp===0.9.1-unreleased) (3.12.4)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp===0.9.1-unreleased) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp===0.9.1-unreleased) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp===0.9.1-unreleased) (1.1.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (0.17)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.2.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.6.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (0.7.12)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (20.9)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.2.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp===0.9.1-unreleased) (3.4.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.1.4)\n",
            "Building wheels for collected packages: allennlp\n",
            "  Building wheel for allennlp (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for allennlp: filename=allennlp-0.9.1_unreleased-cp37-none-any.whl size=7535391 sha256=3ce9f58bd98a8b1f75225f16e960d857522143787c2631d33505bd8e11deacda\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w7_fkoce/wheels/16/b3/9b/fceece1cbc3a6ac0c759db090cb239c3f4cba5bb369bb933c3\n",
            "Successfully built allennlp\n",
            "Building wheels for collected packages: word2number, ftfy, jsonnet, parsimonious\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp37-none-any.whl size=5589 sha256=86fa426dd2dc4ab3fd3bc1d3aafc3ec2bf231f276d9e22d7d3cb26f5026346c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=bf6592e1fc1ed7e80aa175f9c5efdb7343011f068a2d06b5cebeb1af017e60f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388706 sha256=4ce97773edfa17df6cbdd5f5c32cc40c93601523007ea5c28f162f97ca6efad2\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp37-none-any.whl size=42711 sha256=8ef142687a9d65cfb4fbbb0468a62a43e02d6d8768312d7ccd8116eb04175ffa\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "Successfully built word2number ftfy jsonnet parsimonious\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.57 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: responses 0.13.2 has requirement urllib3>=1.25.10, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, conllu, numpydoc, word2number, jsonpickle, ftfy, zope.event, zope.interface, gevent, flask-cors, flaky, responses, jsonnet, sentencepiece, pytorch-transformers, parsimonious, blis, preshed, plac, thinc, spacy, unidecode, typing-utils, overrides, tensorboardX, allennlp\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed allennlp-0.9.1-unreleased blis-0.2.4 boto3-1.17.57 botocore-1.20.57 conllu-1.3.1 flaky-3.7.0 flask-cors-3.0.10 ftfy-6.0.1 gevent-21.1.2 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 numpydoc-1.1.0 overrides-4.1.1 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.13.2 s3transfer-0.4.2 sentencepiece-0.1.95 spacy-2.1.9 tensorboardX-2.2 thinc-7.0.8 typing-utils-0.0.3 unidecode-1.2.0 word2number-1.1 zope.event-4.5.0 zope.interface-5.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsBOTkZqlc8a",
        "outputId": "4d177679-e6cf-41bf-8868-2f03f23127e6"
      },
      "source": [
        "!pip install pytorch-transformers==1.2.0\n",
        "!pip install transformers==2.4.1\n",
        "!pip install transformers==2.6"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 4.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 31.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (1.17.57)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (0.1.95)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (7.1.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers==1.2.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers==1.2.0) (0.4.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.57 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers==1.2.0) (1.20.57)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.57->boto3->pytorch-transformers==1.2.0) (2.8.1)\n",
            "\u001b[31mERROR: allennlp 0.9.1-unreleased has requirement pytorch-transformers==1.1.0, but you'll have pytorch-transformers 1.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sacremoses, pytorch-transformers\n",
            "  Found existing installation: pytorch-transformers 1.1.0\n",
            "    Uninstalling pytorch-transformers-1.1.0:\n",
            "      Successfully uninstalled pytorch-transformers-1.1.0\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.45\n",
            "Collecting transformers==2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.4.1) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.4.1) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.4.1) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.4.1) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.4.1) (4.41.1)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/cb/3e8902d528538972873d0e9e4e47a31d1849a98e057009e9d383637c96fb/tokenizers-0.0.11-cp37-cp37m-manylinux1_x86_64.whl (4.7MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7MB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.4.1) (1.17.57)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.4.1) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.4.1) (0.1.95)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.4.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.4.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.4.1) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.4.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.4.1) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.4.1) (1.15.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.57 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.4.1) (1.20.57)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.4.1) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.4.1) (0.4.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.57->boto3->transformers==2.4.1) (2.8.1)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.0.11 transformers-2.4.1\n",
            "Collecting transformers==2.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.6) (0.1.95)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.6) (1.17.57)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.6) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.6) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.6) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.6) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.6) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.6) (2019.12.20)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 36.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.6) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.6) (0.4.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.57 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.6) (1.20.57)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.6) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.6) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.6) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6) (2.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.57->boto3->transformers==2.6) (2.8.1)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.0.11\n",
            "    Uninstalling tokenizers-0.0.11:\n",
            "      Successfully uninstalled tokenizers-0.0.11\n",
            "  Found existing installation: transformers 2.4.1\n",
            "    Uninstalling transformers-2.4.1:\n",
            "      Successfully uninstalled transformers-2.4.1\n",
            "Successfully installed tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XpRtHVGTafDE",
        "outputId": "1e98367b-aa32-41f8-ebff-e5ba251d04b0"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQt7HfCOl1nE",
        "outputId": "cc7ab9c5-d176-405c-fd79-9f19bc62bd2e"
      },
      "source": [
        "!git clone https://github.com/adapter-hub/adapter-transformers.git\n",
        "%cd adapter-transformers\n",
        "!pip install .\n",
        "!pip install torch==1.7.1\n",
        "%cd /content"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'adapter-transformers'...\n",
            "remote: Enumerating objects: 57013, done.\u001b[K\n",
            "remote: Counting objects: 100% (1959/1959), done.\u001b[K\n",
            "remote: Compressing objects: 100% (914/914), done.\u001b[K\n",
            "remote: Total 57013 (delta 1185), reused 1432 (delta 940), pack-reused 55054\u001b[K\n",
            "Receiving objects: 100% (57013/57013), 33.76 MiB | 22.58 MiB/s, done.\n",
            "Resolving deltas: 100% (41900/41900), done.\n",
            "/content/adapter-transformers\n",
            "Processing /content/adapter-transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (2019.12.20)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ac/f5ba028f0f097d855e1541301e946d4672eb0f30b6e25cb2369075f916d2/tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (20.9)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==1.1.1) (0.0.45)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers==1.1.1) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->adapter-transformers==1.1.1) (56.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->adapter-transformers==1.1.1) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==1.1.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==1.1.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==1.1.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==1.1.1) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==1.1.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==1.1.1) (1.0.1)\n",
            "Building wheels for collected packages: adapter-transformers\n",
            "  Building wheel for adapter-transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adapter-transformers: filename=adapter_transformers-1.1.1-cp37-none-any.whl size=1326452 sha256=f607713af01038fe97c4ae1a48ee270b7eaccd31bd396791ca646338dab33847\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/f2/bd/f0720d145cbd02c42f9d833e06bb319e9ee518b24ebaa092e1\n",
            "Successfully built adapter-transformers\n",
            "\u001b[31mERROR: transformers 2.6.0 has requirement tokenizers==0.5.2, but you'll have tokenizers 0.9.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, adapter-transformers\n",
            "  Found existing installation: tokenizers 0.5.2\n",
            "    Uninstalling tokenizers-0.5.2:\n",
            "      Successfully uninstalled tokenizers-0.5.2\n",
            "Successfully installed adapter-transformers-1.1.1 tokenizers-0.9.3\n",
            "Collecting torch==1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 25kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.5)\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: allennlp 0.9.1-unreleased has requirement pytorch-transformers==1.1.0, but you'll have pytorch-transformers 1.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed torch-1.7.1\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xekSt6x4ldrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a3cd20-aa95-493b-9c4f-0078df659f88"
      },
      "source": [
        "!rm -rf /content/dont-stop-pretraining/\n",
        "!git clone https://github.com/allenai/dont-stop-pretraining.git\n",
        "%cd dont-stop-pretraining/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dont-stop-pretraining'...\n",
            "remote: Enumerating objects: 439, done.\u001b[K\n",
            "remote: Total 439 (delta 0), reused 0 (delta 0), pack-reused 439\u001b[K\n",
            "Receiving objects: 100% (439/439), 566.01 KiB | 2.69 MiB/s, done.\n",
            "Resolving deltas: 100% (232/232), done.\n",
            "/content/dont-stop-pretraining\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VATsGLYasaD",
        "outputId": "339cb8f7-cee9-4142-ca1e-ff07cfccc8ee"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADAPTIVE_PRETRAINING.md  \u001b[0m\u001b[01;34menvironments\u001b[0m/    README.md    \u001b[01;34msearch_space\u001b[0m/\n",
            "DATA_SELECTION.md        environment.yml  scatter.pdf  \u001b[01;34mtraining_config\u001b[0m/\n",
            "\u001b[01;34mdont_stop_pretraining\u001b[0m/   \u001b[01;34mmlm_study\u001b[0m/       \u001b[01;34mscripts\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wni1g3na8EB",
        "outputId": "7c91f58b-8218-49e5-ec12-5b5dc791721c"
      },
      "source": [
        "%cd dont-stop-pretraining/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'dont-stop-pretraining/'\n",
            "/content/dont-stop-pretraining\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkZN3g0ma1AT",
        "outputId": "eaa5264b-c8d5-4c9a-8de6-76c0431126c9"
      },
      "source": [
        "!python -m scripts.download_model \\\n",
        "        --model allenai/dsp_roberta_base_dapt_cs_tapt_citation_intent_1688 \\\n",
        "        --serialization_dir ./pretrained_models/dsp_roberta_base_dapt_cs_tapt_citation_intent_1688"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-25 08:13:05.205143: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Downloading: 100% 430/430 [00:00<00:00, 404kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.40MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.46MB/s]\n",
            "Downloading: 100% 2.00/2.00 [00:00<00:00, 1.75kB/s]\n",
            "Downloading: 100% 150/150 [00:00<00:00, 155kB/s]\n",
            "Downloading: 100% 185/185 [00:00<00:00, 178kB/s]\n",
            "Downloading: 100% 656M/656M [00:09<00:00, 67.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-yqvgp5l548"
      },
      "source": [
        "import torch\n",
        "from transformers import RobertaModel\n",
        "import transformers\n",
        "import os\n",
        "\n",
        "serialization_dir = \"./pretrained_models/test_roberta_model_with_adapters\"\n",
        "\n",
        "if not os.path.isdir(serialization_dir):\n",
        "        os.makedirs(serialization_dir)\n",
        "\n",
        "\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "adapter_name = 'skj'\n",
        "config = transformers.AdapterConfig.load(\"pfeiffer\", reduction_factor=12)\n",
        "model.add_adapter(adapter_name, transformers.AdapterType.text_task, config=config)\n",
        "#model.train_adapter([adapter_name])\n",
        "#model.save_pretrained(\"./pretrained_models/test_roberta_model_with_adapters\")\n",
        "model.save_adapter('./skj_adapter/', adapter_name)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EF00r6cnlFw"
      },
      "source": [
        "adapter_name = 'skj-lang'\n",
        "model.add_adapter(adapter_name, transformers.AdapterType.text_lang, config=config)\n",
        "model.save_adapter('./skj-lan_adapter/', adapter_name)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ_MveFDm3OM",
        "outputId": "dc5db2da-4ebb-436e-fcac-bebb45b81aca"
      },
      "source": [
        "# citation_intent\n",
        "!curl -Lo ./data/citation_intent/train.jsonl --create-dirs https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/train.jsonl\n",
        "!curl -Lo ./data/citation_intent/dev.jsonl --create-dirs https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/dev.jsonl\n",
        "!curl -Lo ./data/citation_intent/test.jsonl --create-dirs https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/test.jsonl"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  454k  100  454k    0     0   350k      0  0:00:01  0:00:01 --:--:--  350k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 30138  100 30138    0     0  42447      0 --:--:-- --:--:-- --:--:-- 42388\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 38637  100 38637    0     0  55914      0 --:--:-- --:--:-- --:--:-- 55833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSR16k-ZlgBM",
        "outputId": "3bfb3ebf-8297-4c41-b863-4e3492a7167e"
      },
      "source": [
        "!python ./adapter-transformers/examples/contrib/legacy/run_language_modeling.py \\\n",
        "  --train_data_file ./data/citation_intent/train.jsonl \\\n",
        "  --line_by_line \\\n",
        "  --output_dir roberta-adapter-citation_intent-tapt \\\n",
        "  --model_type roberta-base \\\n",
        "  --tokenizer_name roberta-base \\\n",
        "  --mlm \\\n",
        "  --per_gpu_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --model_name_or_path ./pretrained_models/test_roberta_model_with_adapters \\\n",
        "  --eval_data_file ./data/citation_intent/dev.jsonl \\\n",
        "  --do_eval \\\n",
        "  --evaluate_during_training \\\n",
        "  --do_train \\\n",
        "  --num_train_epochs 10 \\\n",
        "  --learning_rate 0.0001 \\\n",
        "  --logging_steps 50 \\\n",
        "  --language en \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-24 19:35:19.911420: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:347: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "04/24/2021 19:35:21 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/24/2021 19:35:21 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='roberta-adapter-citation_intent-tapt', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Apr24_19-35-21_0fc2402d31e8', logging_first_step=False, logging_steps=50, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='roberta-adapter-citation_intent-tapt', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_auto.py:1001: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at ./pretrained_models/test_roberta_model_with_adapters and are newly initialized: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:114: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:285: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
            "  FutureWarning,\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{'loss': 12.144012451171875, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.30188679245283}\n",
            " 83% 50/60 [06:01<01:14,  7.48s/it]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 23.85it/s]\u001b[A\n",
            " 27% 4/15 [00:00<00:00, 15.32it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 15.35it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 12.33it/s]\u001b[A\n",
            " 60% 9/15 [00:00<00:00, 12.59it/s]\u001b[A\n",
            " 73% 11/15 [00:00<00:00, 11.31it/s]\u001b[A\n",
            " 87% 13/15 [00:01<00:00, 11.37it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 7.804431438446045, 'epoch': 8.30188679245283}\n",
            " 83% 50/60 [06:02<01:14,  7.48s/it]\n",
            "100% 15/15 [00:01<00:00, 10.35it/s]\u001b[A\n",
            "{'epoch': 9.90566037735849}\n",
            "100% 60/60 [07:13<00:00,  7.23s/it]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1219: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
            "04/24/2021 19:42:45 - INFO - __main__ -   *** Evaluate ***\n",
            "100% 15/15 [00:01<00:00, 11.64it/s]\n",
            "04/24/2021 19:42:46 - INFO - __main__ -   ***** Eval results *****\n",
            "04/24/2021 19:42:46 - INFO - __main__ -     perplexity = 1952.3096728820371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoytF-vHnB0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e982e2-8dfd-4ce0-a545-1c729afcf253"
      },
      "source": [
        "!python ./adapter-transformers/examples/contrib/legacy/run_language_modeling.py \\\n",
        "  --train_data_file ./data/citation_intent/train.jsonl \\\n",
        "  --line_by_line \\\n",
        "  --output_dir roberta-adapter-citation_intent-tapt_2 \\\n",
        "  --model_type roberta-base \\\n",
        "  --tokenizer_name roberta-base \\\n",
        "  --mlm \\\n",
        "  --per_gpu_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --model_name_or_path roberta-base \\\n",
        "  --eval_data_file ./data/citation_intent/dev.jsonl \\\n",
        "  --do_eval \\\n",
        "  --evaluate_during_training \\\n",
        "  --do_train \\\n",
        "  --num_train_epochs 10 \\\n",
        "  --learning_rate 0.0001 \\\n",
        "  --logging_steps 50 \\\n",
        "  --language en \\\n",
        "  --load_adapter skj \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-24 20:23:17.509388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:347: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "04/24/2021 20:23:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/24/2021 20:23:19 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='roberta-adapter-citation_intent-tapt_2', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Apr24_20-23-19_0fc2402d31e8', logging_first_step=False, logging_steps=50, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='roberta-adapter-citation_intent-tapt_2', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_auto.py:1001: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Traceback (most recent call last):\n",
            "  File \"./adapter-transformers/examples/contrib/legacy/run_language_modeling.py\", line 387, in <module>\n",
            "    main()\n",
            "  File \"./adapter-transformers/examples/contrib/legacy/run_language_modeling.py\", line 286, in main\n",
            "    load_as=language,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapter_model_mixin.py\", line 1124, in load_adapter\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapter_model_mixin.py\", line 943, in load_adapter\n",
            "    load_dir, load_name = loader.load(adapter_name_or_path, config, version, model_name, load_as, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapter_model_mixin.py\", line 389, in load\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/adapter_utils.py\", line 439, in resolve_adapter_path\n",
            "    raise ValueError(\"Unable to identify {} as a valid module location.\".format(adapter_name_or_path))\n",
            "ValueError: Unable to identify [skj] as a valid module location.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjjJ5Cfbb1r6",
        "outputId": "589abbf2-3fc2-486d-dfdd-14b466d50016"
      },
      "source": [
        "!python ./adapter-transformers/examples/contrib/legacy/run_language_modeling.py \\\n",
        "  --train_data_file ./data/citation_intent/train.jsonl \\\n",
        "  --line_by_line \\\n",
        "  --output_dir roberta-adapter-citation_intent-tapt_1 \\\n",
        "  --model_type roberta-base \\\n",
        "  --tokenizer_name roberta-base \\\n",
        "  --mlm \\\n",
        "  --per_gpu_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --model_name_or_path ./pretrained_models/dsp_roberta_base_dapt_cs_tapt_citation_intent_1688 \\\n",
        "  --eval_data_file ./data/citation_intent/dev.jsonl \\\n",
        "  --do_eval \\\n",
        "  --evaluate_during_training \\\n",
        "  --do_train \\\n",
        "  --num_train_epochs 10 \\\n",
        "  --learning_rate 0.0001 \\\n",
        "  --logging_steps 50 \\\n",
        "  --language en \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-24 19:52:18.479807: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:347: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "04/24/2021 19:52:20 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/24/2021 19:52:20 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='roberta-adapter-citation_intent-tapt_1', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Apr24_19-52-20_0fc2402d31e8', logging_first_step=False, logging_steps=50, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='roberta-adapter-citation_intent-tapt_1', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_auto.py:1001: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at ./pretrained_models/dsp_roberta_base_dapt_cs_tapt_citation_intent_1688 and are newly initialized: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:114: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:285: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
            "  FutureWarning,\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{'loss': 11.170086669921876, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.30188679245283}\n",
            " 83% 50/60 [06:15<01:17,  7.75s/it]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 3/15 [00:00<00:00, 23.63it/s]\u001b[A\n",
            " 27% 4/15 [00:00<00:00, 14.95it/s]\u001b[A\n",
            " 40% 6/15 [00:00<00:00, 14.85it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 11.89it/s]\u001b[A\n",
            " 60% 9/15 [00:00<00:00, 12.28it/s]\u001b[A\n",
            " 73% 11/15 [00:00<00:00, 10.99it/s]\u001b[A\n",
            " 87% 13/15 [00:01<00:00, 10.94it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 6.926140308380127, 'epoch': 8.30188679245283}\n",
            " 83% 50/60 [06:16<01:17,  7.75s/it]\n",
            "100% 15/15 [00:01<00:00, 10.03it/s]\u001b[A\n",
            "{'epoch': 9.90566037735849}\n",
            "100% 60/60 [07:30<00:00,  7.50s/it]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1219: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
            "04/24/2021 20:00:02 - INFO - __main__ -   *** Evaluate ***\n",
            "100% 15/15 [00:01<00:00, 11.19it/s]\n",
            "04/24/2021 20:00:03 - INFO - __main__ -   ***** Eval results *****\n",
            "04/24/2021 20:00:03 - INFO - __main__ -     perplexity = 900.662091281961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy8HfP-WcAoX",
        "outputId": "a7d30067-a5f3-4467-f575-71fefc48c28a"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXw6mSFx5piw",
        "outputId": "3a098847-6be1-4114-b317-6919607ce368"
      },
      "source": [
        "%cd content/"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'content/'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEg-Tvn20Jif",
        "outputId": "cd0943d2-19ba-4368-d5ff-408ca6929700"
      },
      "source": [
        "!pip install allennlp-models\n",
        "!from allennlp_models import pretrained\n",
        "#print(pretrained.get_pretrained_models())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allennlp-models in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: torch<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.7.1)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (3.2.5)\n",
            "Requirement already satisfied: py-rouge==1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.1)\n",
            "Requirement already satisfied: conllu==4.4 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (4.4)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (6.0.1)\n",
            "Requirement already satisfied: allennlp<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch<1.9.0,>=1.7.0->allennlp-models) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9.0,>=1.7.0->allennlp-models) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp-models) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp-models) (0.2.5)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (1.17.57)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (1.4.1)\n",
            "Requirement already satisfied: wandb<0.11.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.10.27)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.1.95)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (8.7.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.99)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (2.10.0)\n",
            "Requirement already satisfied: transformers<4.6,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (4.5.1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (4.41.1)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.22.2.post1)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (3.0.12)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (2.23.0)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (2.1.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.0.8)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (3.6.4)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (2.2)\n",
            "Requirement already satisfied: torchvision<0.10.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.9.1+cu101)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.57 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.5,>=2.4.0->allennlp-models) (1.20.57)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.5,>=2.4.0->allennlp-models) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.5,>=2.4.0->allennlp-models) (0.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (2.8.1)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (1.0.1)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (3.5.4)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (3.12.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (2.3)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (3.13)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (0.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (3.1.14)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (5.0.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (0.1.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (3.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (2019.12.20)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp<2.5,>=2.4.0->allennlp-models) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.5,>=2.4.0->allennlp-models) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.5,>=2.4.0->allennlp-models) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.5,>=2.4.0->allennlp-models) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.5,>=2.4.0->allennlp-models) (1.24.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (2.0.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (0.9.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (0.8.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (7.0.8)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.5,>=2.4.0->allennlp-models) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.5,>=2.4.0->allennlp-models) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.5,>=2.4.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.5,>=2.4.0->allennlp-models) (20.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.5,>=2.4.0->allennlp-models) (56.0.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.10.0,>=0.8.1->allennlp<2.5,>=2.4.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (4.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (2.4.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (4.0.0)\n",
            "from: can't read /var/mail/allennlp_models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAjHefRipTFQ",
        "outputId": "c3bf4a52-432e-441e-c731-b4141519a6b5"
      },
      "source": [
        "!python ./adapter-transformers/examples/contrib/legacy/run_language_modeling.py \\\n",
        "  --train_data_file ./data/citation_intent/train.jsonl \\\n",
        "  --line_by_line \\\n",
        "  --output_dir roberta-adapter-citation_intent-cs \\\n",
        "  --model_type roberta-base \\\n",
        "  --tokenizer_name roberta-base \\\n",
        "  --mlm \\\n",
        "  --per_gpu_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --model_name_or_path allenai/cs_roberta_base \\\n",
        "  --eval_data_file ./data/citation_intent/dev.jsonl \\\n",
        "  --do_eval \\\n",
        "  --evaluate_during_training \\\n",
        "  --do_train \\\n",
        "  --num_train_epochs 50 \\\n",
        "  --learning_rate 0.001 \\\n",
        "  --logging_steps 50 \\\n",
        "  --language en \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-25 09:25:12.252545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:347: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "04/25/2021 09:25:13 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/25/2021 09:25:13 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='roberta-adapter-citation_intent-cs', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=0.001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Apr25_09-25-13_6fc8be44aca6', logging_first_step=False, logging_steps=50, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='roberta-adapter-citation_intent-cs', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_auto.py:1001: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at allenai/cs_roberta_base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:114: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:285: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
            "  FutureWarning,\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{'loss': 5.083323669433594, 'learning_rate': 0.0008333333333333334, 'epoch': 8.30188679245283}\n",
            " 17% 50/300 [03:22<17:02,  4.09s/it]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 27% 4/15 [00:00<00:00, 29.52it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 27.55it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 27.19it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 22.98it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 3.191037654876709, 'epoch': 8.30188679245283}\n",
            " 17% 50/300 [03:22<17:02,  4.09s/it]\n",
            "100% 15/15 [00:00<00:00, 21.90it/s]\u001b[A\n",
            "{'loss': 3.9442825317382812, 'learning_rate': 0.0006666666666666666, 'epoch': 16.60377358490566}\n",
            " 33% 100/300 [06:43<13:15,  3.98s/it]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 27% 4/15 [00:00<00:00, 29.23it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 27.42it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 27.01it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 23.04it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.939610004425049, 'epoch': 16.60377358490566}\n",
            " 33% 100/300 [06:43<13:15,  3.98s/it]\n",
            "100% 15/15 [00:00<00:00, 21.85it/s]\u001b[A\n",
            "{'loss': 3.714765625, 'learning_rate': 0.0005, 'epoch': 24.90566037735849}\n",
            " 50% 150/300 [10:03<09:27,  3.79s/it]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 27% 4/15 [00:00<00:00, 29.32it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 27.42it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 27.10it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 22.89it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.6857268810272217, 'epoch': 24.90566037735849}\n",
            " 50% 150/300 [10:04<09:27,  3.79s/it]\n",
            "100% 15/15 [00:00<00:00, 21.94it/s]\u001b[A\n",
            "{'loss': 3.60870849609375, 'learning_rate': 0.0003333333333333333, 'epoch': 33.301886792452834}\n",
            " 67% 200/300 [13:28<07:09,  4.30s/it]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 27% 4/15 [00:00<00:00, 29.31it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 27.04it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 27.11it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 23.28it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.750255584716797, 'epoch': 33.301886792452834}\n",
            " 67% 200/300 [13:29<07:09,  4.30s/it]\n",
            "100% 15/15 [00:00<00:00, 21.95it/s]\u001b[A\n",
            "{'loss': 3.42664794921875, 'learning_rate': 0.00016666666666666666, 'epoch': 41.60377358490566}\n",
            " 83% 250/300 [16:46<03:13,  3.87s/it]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 27% 4/15 [00:00<00:00, 29.55it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 27.52it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 27.22it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 23.17it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.7641069889068604, 'epoch': 41.60377358490566}\n",
            " 83% 250/300 [16:47<03:13,  3.87s/it]\n",
            "100% 15/15 [00:00<00:00, 21.91it/s]\u001b[A\n",
            "{'loss': 3.396182861328125, 'learning_rate': 0.0, 'epoch': 49.905660377358494}\n",
            "100% 300/300 [20:06<00:00,  3.90s/it]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 27% 4/15 [00:00<00:00, 29.34it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 27.42it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 27.08it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 23.12it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.5555055141448975, 'epoch': 49.905660377358494}\n",
            "100% 300/300 [20:07<00:00,  3.90s/it]\n",
            "100% 15/15 [00:00<00:00, 21.88it/s]\u001b[A\n",
            "{'epoch': 49.905660377358494}\n",
            "100% 300/300 [20:07<00:00,  4.03s/it]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1219: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
            "04/25/2021 09:45:30 - INFO - __main__ -   *** Evaluate ***\n",
            "100% 15/15 [00:00<00:00, 23.16it/s]\n",
            "04/25/2021 09:45:31 - INFO - __main__ -   ***** Eval results *****\n",
            "04/25/2021 09:45:31 - INFO - __main__ -     perplexity = 14.236406026177429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVI0A_ix5Lae",
        "outputId": "ddcf3ead-a67a-4409-d11c-1eee5f509060"
      },
      "source": [
        "#download the pretrained RoBerta based CS model\n",
        "!python -m dont-stop-pretraining.scripts.download_model \\\n",
        "        --model allenai/cs_roberta_base \\\n",
        "        --serialization_dir ./pretrained_models/cs_roberta_base"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-25 10:48:51.416919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 846kB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 514kB/s]\n",
            "Downloading: 100% 2.00/2.00 [00:00<00:00, 1.47kB/s]\n",
            "Downloading: 100% 150/150 [00:00<00:00, 69.3kB/s]\n",
            "Downloading: 100% 185/185 [00:00<00:00, 154kB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph3pIvAsIPtp",
        "outputId": "e1cd8d02-d692-48ca-d3a9-ec03b4711da6"
      },
      "source": [
        "!python ./adapter-transformers/examples/contrib/legacy/run_language_modeling.py \\\n",
        "  --train_data_file ./data/citation_intent/train.jsonl \\\n",
        "  --line_by_line \\\n",
        "  --overwrite_output_dir \\\n",
        "  --output_dir roberta-adapter-citation_intent-tapt_2 \\\n",
        "  --model_type roberta-base \\\n",
        "  --tokenizer_name roberta-base \\\n",
        "  --mlm \\\n",
        "  --per_gpu_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --model_name_or_path ./pretrained_models/cs_roberta_base \\\n",
        "  --eval_data_file ./data/citation_intent/dev.jsonl \\\n",
        "  --do_eval \\\n",
        "  --evaluate_during_training \\\n",
        "  --do_train \\\n",
        "  --num_train_epochs 10 \\\n",
        "  --learning_rate 0.0001 \\\n",
        "  --logging_steps 50 \\\n",
        "  --language en \\\n",
        "  --train_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-25 08:34:15.610974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:347: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "04/25/2021 08:34:17 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/25/2021 08:34:17 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='roberta-adapter-citation_intent-tapt_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Apr25_08-34-17_6fc8be44aca6', logging_first_step=False, logging_steps=50, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='roberta-adapter-citation_intent-tapt_2', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_auto.py:1001: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at ./pretrained_models/cs_roberta_base and are newly initialized: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:114: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:285: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
            "  FutureWarning,\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{'loss': 9.22533935546875, 'learning_rate': 1.6666666666666667e-05, 'epoch': 8.30188679245283}\n",
            " 83% 50/60 [03:22<00:40,  4.09s/it]\n",
            "  0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            " 27% 4/15 [00:00<00:00, 29.39it/s]\u001b[A\n",
            " 47% 7/15 [00:00<00:00, 27.44it/s]\u001b[A\n",
            " 67% 10/15 [00:00<00:00, 27.16it/s]\u001b[A\n",
            " 80% 12/15 [00:00<00:00, 23.21it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 4.783523082733154, 'epoch': 8.30188679245283}\n",
            " 83% 50/60 [03:22<00:40,  4.09s/it]\n",
            "100% 15/15 [00:00<00:00, 21.96it/s]\u001b[A\n",
            "{'epoch': 9.90566037735849}\n",
            "100% 60/60 [04:01<00:00,  4.02s/it]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1219: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n",
            "04/25/2021 08:38:27 - INFO - __main__ -   *** Evaluate ***\n",
            "100% 15/15 [00:00<00:00, 23.15it/s]\n",
            "04/25/2021 08:38:28 - INFO - __main__ -   ***** Eval results *****\n",
            "04/25/2021 08:38:28 - INFO - __main__ -     perplexity = 108.28950626387882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnuyDFGCLsMo",
        "outputId": "abda80b4-da61-486c-b82a-3626f0ec9409"
      },
      "source": [
        "!python ./adapter-transformers/examples/contrib/legacy/run_language_modeling.py \\\n",
        "  --train_data_file ./data/citation_intent/train.jsonl \\\n",
        "  --line_by_line \\\n",
        "  --overwrite_output_dir \\\n",
        "  --output_dir roberta-adapter-citation_intent-tapt_1 \\\n",
        "  --model_type roberta-base \\\n",
        "  --tokenizer_name roberta-base \\\n",
        "  --mlm \\\n",
        "  --per_gpu_train_batch_size 256 \\\n",
        "  --gradient_accumulation_steps 256 \\\n",
        "  --model_name_or_path ./pretrained_models/cs_roberta_base \\\n",
        "  --eval_data_file ./data/citation_intent/dev.jsonl \\\n",
        "  --do_eval \\\n",
        "  --evaluate_during_training \\\n",
        "  --do_train \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --learning_rate 0.0001 \\\n",
        "  --logging_steps 50 \\\n",
        "  --adapter_non_linearity tanh \\\n",
        "  --weight_decay 0.1 \\\n",
        "  --language en \\\n",
        "  --train_adapter \\\n",
        "  --load_adapter ./skj-lan_adapter \\\n",
        "  --adapter_config pfeiffer"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-25 11:22:44.026613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:347: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n",
            "04/25/2021 11:22:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/25/2021 11:22:46 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='roberta-adapter-citation_intent-tapt_1', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=256, per_gpu_eval_batch_size=None, gradient_accumulation_steps=256, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.1, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Apr25_11-22-46_39f0dac72874', logging_first_step=False, logging_steps=50, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='roberta-adapter-citation_intent-tapt_1', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "model path ./pretrained_models/cs_roberta_base\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_auto.py:1001: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at ./pretrained_models/cs_roberta_base and are newly initialized: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "language en\n",
            "model.config.adapters.adapter_list(AdapterType.text_lang) []\n",
            "AdapterType.text_lang AdapterType.text_lang\n",
            "adapter_args.load_adapter ./skj-lan_adapter\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:114: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:285: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
            "  FutureWarning,\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "  0% 0/1 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"./adapter-transformers/examples/contrib/legacy/run_language_modeling.py\", line 393, in <module>\n",
            "    main()\n",
            "  File \"./adapter-transformers/examples/contrib/legacy/run_language_modeling.py\", line 357, in main\n",
            "    trainer.train(model_path=model_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 787, in train\n",
            "    tr_loss += self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1157, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1181, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_roberta.py\", line 1024, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_roberta.py\", line 730, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_roberta.py\", line 452, in forward\n",
            "    adapter_names=adapter_names,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_roberta.py\", line 389, in forward\n",
            "    adapter_names=adapter_names,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 1712, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors, adapter_names=adapter_names)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_roberta.py\", line 395, in feed_forward_chunk\n",
            "    intermediate_output = self.intermediate(attention_output)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_roberta.py\", line 312, in forward\n",
            "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1383, in gelu\n",
            "    return torch._C._nn.gelu(input)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 694.00 MiB (GPU 0; 11.17 GiB total capacity; 9.87 GiB already allocated; 649.31 MiB free; 10.04 GiB reserved in total by PyTorch)\n",
            "  0% 0/1 [00:01<?, ?it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-kOJqKgkjeV",
        "outputId": "74ede0e3-8665-4d34-b05a-e3d3676e01a6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "juL1cab3vwo-",
        "outputId": "6104326c-e24b-4fe2-d450-2e8d822bf8e6"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}